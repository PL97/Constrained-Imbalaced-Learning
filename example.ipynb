{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "under-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except:\n",
    "    ! pip install tensorflow\n",
    "    import tensorflow as tf\n",
    "\n",
    "# Use the GitHub version of TFCO\n",
    "try:\n",
    "    import tensorflow_constrained_optimization as tfco\n",
    "except:\n",
    "    !pip install git+https://github.com/google-research/tensorflow_constrained_optimization\n",
    "    import tensorflow_constrained_optimization as tfco\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrong-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulated 10-dimensional training dataset consisting of 1000 labeled\n",
    "# examples, of which 800 are labeled correctly and 200 are mislabeled.\n",
    "num_examples = 1000\n",
    "num_mislabeled_examples = 200\n",
    "dimension = 10\n",
    "# We will constrain the recall to be at least 90%.\n",
    "recall_lower_bound = 0.9\n",
    "\n",
    "# Create random \"ground truth\" parameters for a linear model.\n",
    "ground_truth_weights = np.random.normal(size=dimension) / math.sqrt(dimension)\n",
    "ground_truth_threshold = 0\n",
    "\n",
    "# Generate a random set of features for each example.\n",
    "features = np.random.normal(size=(num_examples, dimension)).astype(\n",
    "    np.float32) / math.sqrt(dimension)\n",
    "# Compute the labels from these features given the ground truth linear model.\n",
    "labels = (np.matmul(features, ground_truth_weights) >\n",
    "          ground_truth_threshold).astype(np.float32)\n",
    "# Add noise by randomly flipping num_mislabeled_examples labels.\n",
    "mislabeled_indices = np.random.choice(\n",
    "    num_examples, num_mislabeled_examples, replace=False)\n",
    "labels[mislabeled_indices] = 1 - labels[mislabeled_indices]\n",
    "\n",
    "# Constant Tensors containing the labels and features.\n",
    "constant_labels = tf.constant(labels, dtype=tf.float32)\n",
    "constant_features = tf.constant(features, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "composite-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables containing the model parameters.\n",
    "weights = tf.Variable(tf.zeros(dimension), dtype=tf.float32, name=\"weights\")\n",
    "threshold = tf.Variable(0.0, dtype=tf.float32, name=\"threshold\")\n",
    "\n",
    "# The predictions are a nullary function returning a Tensor to support eager\n",
    "# mode. In graph mode, you can simply use:\n",
    "#   predictions = (\n",
    "#       tf.tensordot(constant_features, weights, axes=(1, 0)) - threshold)\n",
    "def predictions():\n",
    "    return tf.tensordot(constant_features, weights, axes=(1, 0)) - threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "indie-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like the predictions, in eager mode, the labels should be a nullary function\n",
    "# returning a Tensor. In graph mode, you can drop the lambda.\n",
    "context = tfco.rate_context(predictions, labels=lambda: constant_labels)\n",
    "problem = tfco.RateMinimizationProblem(\n",
    "    tfco.error_rate(context), [tfco.recall(context) >= recall_lower_bound])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "identical-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleProblem(tfco.ConstrainedMinimizationProblem):\n",
    "\n",
    "    def __init__(self, labels, predictions, recall_lower_bound):\n",
    "        self._labels = labels\n",
    "        self._predictions = predictions\n",
    "        self._recall_lower_bound = recall_lower_bound\n",
    "        # The number of positively-labeled examples.\n",
    "        self._positive_count = tf.reduce_sum(self._labels)\n",
    "\n",
    "    @property\n",
    "    def num_constraints(self):\n",
    "        return 1\n",
    "\n",
    "    def objective(self):\n",
    "        # In eager mode, the predictions must be a nullary function returning a\n",
    "        # Tensor. In graph mode, they could be either such a function, or a Tensor\n",
    "        # itself.\n",
    "        predictions = self._predictions\n",
    "        if callable(predictions):\n",
    "            predictions = predictions()\n",
    "        return tf.compat.v1.losses.hinge_loss(labels=self._labels,\n",
    "                                              logits=predictions)\n",
    "\n",
    "    def constraints(self):\n",
    "        # In eager mode, the predictions must be a nullary function returning a\n",
    "        # Tensor. In graph mode, they could be either such a function, or a Tensor\n",
    "        # itself.\n",
    "        predictions = self._predictions\n",
    "        if callable(predictions):\n",
    "            predictions = predictions()\n",
    "        # Recall that the labels are binary (0 or 1).\n",
    "        true_positives = self._labels * tf.cast(predictions > 0, dtype=tf.float32)\n",
    "        true_positive_count = tf.reduce_sum(true_positives)\n",
    "        recall = true_positive_count / self._positive_count\n",
    "        # The constraint is (recall >= self._recall_lower_bound), which we convert\n",
    "        # to (self._recall_lower_bound - recall <= 0) because\n",
    "        # ConstrainedMinimizationProblems must always provide their constraints in\n",
    "        # the form (tensor <= 0).\n",
    "        #\n",
    "        # The result of this function should be a tensor, with each element being\n",
    "        # a quantity that is constrained to be non-positive. We only have one\n",
    "        # constraint, so we return a one-element tensor.\n",
    "        return self._recall_lower_bound - recall\n",
    "\n",
    "    def proxy_constraints(self):\n",
    "        # In eager mode, the predictions must be a nullary function returning a\n",
    "        # Tensor. In graph mode, they could be either such a function, or a Tensor\n",
    "        # itself.\n",
    "        predictions = self._predictions\n",
    "        if callable(predictions):\n",
    "            predictions = predictions()\n",
    "        # Use 1 - hinge since we're SUBTRACTING recall in the constraint function,\n",
    "        # and we want the proxy constraint function to be convex. Recall that the\n",
    "        # labels are binary (0 or 1).\n",
    "        true_positives = self._labels * tf.minimum(1.0, predictions)\n",
    "        true_positive_count = tf.reduce_sum(true_positives)\n",
    "        recall = true_positive_count / self._positive_count\n",
    "        # Please see the corresponding comment in the constraints property.\n",
    "        return self._recall_lower_bound - recall\n",
    "\n",
    "problem = ExampleProblem(\n",
    "    labels=constant_labels,\n",
    "    predictions=predictions,\n",
    "    recall_lower_bound=recall_lower_bound,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "starting-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_hinge_loss(labels, predictions):\n",
    "    # Recall that the labels are binary (0 or 1).\n",
    "    signed_labels = (labels * 2) - 1\n",
    "    return np.mean(np.maximum(0.0, 1.0 - signed_labels * predictions))\n",
    "\n",
    "def recall(labels, predictions):\n",
    "    # Recall that the labels are binary (0 or 1).\n",
    "    positive_count = np.sum(labels)\n",
    "    true_positives = labels * (predictions > 0)\n",
    "    true_positive_count = np.sum(true_positives)\n",
    "    return true_positive_count / positive_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "temporal-commodity",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c535c31bdc37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     train = tf.keras.optimizers.Adam().minimize(cost(), var_list=[W, b],tape=tf.GradientTape())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    524\u001b[0m           \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \"\"\"\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    257\u001b[0m                     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \"\"\"\n\u001b[1;32m   1051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n\u001b[0m\u001b[1;32m   1053\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1054\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"
     ]
    }
   ],
   "source": [
    "if tf.executing_eagerly():\n",
    "    # In eager mode, we use a V2 optimizer (a tf.keras.optimizers.Optimizer). A V1\n",
    "    # optimizer, however, would work equally well.\n",
    "    optimizer = tfco.ProxyLagrangianOptimizerV2(\n",
    "                optimizer=tf.keras.optimizers.Adagrad(learning_rate=1.0),\n",
    "                  num_constraints=problem.num_constraints)\n",
    "    # In addition to the model parameters (weights and threshold), we also need to\n",
    "    # optimize over any trainable variables associated with the problem (e.g.\n",
    "    # implicit slack variables and weight denominators), and those associated with\n",
    "    # the optimizer (the analogues of the Lagrange multipliers used by the\n",
    "    # proxy-Lagrangian formulation).\n",
    "    var_list = ([weights, threshold] + list(problem.trainable_variables) +\n",
    "              optimizer.trainable_variables())\n",
    "\n",
    "#     print(var_list)\n",
    "#     print(optimizer)\n",
    "#     train = tf.keras.optimizers.Adam().minimize(cost(), var_list=[W, b],tape=tf.GradientTape())\n",
    "    for ii in xrange(1000):\n",
    "        optimizer.minimize(problem, var_list=var_list, tape=tf.GradientTape())\n",
    "\n",
    "    trained_weights = weights.numpy()\n",
    "    trained_threshold = threshold.numpy()\n",
    "\n",
    "else:  # We're in graph mode.\n",
    "    # In graph mode, we use a V1 optimizer (a tf.compat.v1.train.Optimizer). A V2\n",
    "    # optimizer, however, would work equally well.\n",
    "    optimizer = tfco.ProxyLagrangianOptimizerV1(\n",
    "      optimizer=tf.compat.v1.train.AdagradOptimizer(learning_rate=1.0))\n",
    "    train_op = optimizer.minimize(problem)\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        for ii in xrange(1000):\n",
    "            session.run(train_op)\n",
    "\n",
    "    trained_weights, trained_threshold = session.run((weights, threshold))\n",
    "\n",
    "trained_predictions = np.matmul(features, trained_weights) - trained_threshold\n",
    "print(\"Constrained average hinge loss = %f\" %\n",
    "      average_hinge_loss(labels, trained_predictions))\n",
    "print(\"Constrained recall = %f\" % recall(labels, trained_predictions))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-february",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-calvin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
